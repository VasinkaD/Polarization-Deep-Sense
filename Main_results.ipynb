{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7a8b4f-3050-42fd-a855-570705e87f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#Use \"-1\" for CPU-only devices\n",
    "\n",
    "#If working on GPU enviroment\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#Limit the memory allocated by tensorflow to only the necessary amount\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['PYTHONHASHSEED'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fc1b05-79db-429b-82ca-cc5b737a2115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.1 when it was built against 1.14.0, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages are using the targeted versions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(18)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "set_seed(18)\n",
    "set_random_seed(18)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Supporting_func_file import *\n",
    "\n",
    "Version_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa74fe6-d419-4732-9d68-d2da10a47769",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "#### Loading the 30,000 measured samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d85855-3fc1-4c6f-abc3-6348deb66341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "Count distribution of 7 detectors:            (30000, 7)\n",
      "Corresponding coherence matrices:             (30000, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Data_sets/Performance_evaluation/Performance_evaluation_50_ms_data.npz\"\n",
    "\n",
    "fiber_bundle_counts, input_states_rho = Load_data_file(file_path)\n",
    "\n",
    "print(\"Shapes\")\n",
    "print(\"Count distribution of 7 detectors:           \", fiber_bundle_counts.shape)\n",
    "print(\"Corresponding coherence matrices:            \", input_states_rho.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca58ae-dfc0-4fe1-865d-fba16f7e4826",
   "metadata": {},
   "source": [
    "#### Generating the mixed state samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8918255f-8101-4d69-be5c-27f5765c8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_repetitions = 20\n",
    "#The created dataset will consists of (number_of_repetitions * 30000) generated mixed data covering the whole Bloch sphere\n",
    "\n",
    "radial_positions = Radius_dist((number_of_repetitions, input_states_rho.shape[0]))\n",
    "#Supporting variable for generating mixed states\n",
    "\n",
    "data_rhos, data_counts = Generate_mixed_dataset(input_states_rho, fiber_bundle_counts, radial_positions)\n",
    "#Dataset of coherence matrices (data_rhos) with corresponding count distributions (data_counts) generated from the 30,000 measured samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6e6c8-4d0f-4baf-b748-5a2064457999",
   "metadata": {},
   "source": [
    "#### Pre-processing the data for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e900ae9-30f8-4a8e-8dbe-35ee7fdfe61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First coherency matrix:\n",
      "[[0.497+0.j   0.303-0.31j]\n",
      " [0.303+0.31j 0.503-0.j  ]]\n",
      "\n",
      "Its Cholesky decomposition in the form of a triangular matrix, Tau:\n",
      "[[0.705+0.j    0.   +0.j   ]\n",
      " [0.43 +0.439j 0.354+0.j   ]]\n",
      "\n",
      "The 4 real-valued elements of the Cholesky decomposition, Flat:\n",
      "[0.705 0.43  0.439 0.354]\n"
     ]
    }
   ],
   "source": [
    "data_in = Probability_norm(data_counts)\n",
    "#Network inputs are the count distributions of the polarization state normed to represent the relative frequencies of the 7 detection channels. I.e., for first polarization state, sum(data_in[0,:7]) = 1.\n",
    "\n",
    "data_out = Tau_to_Flat(np.linalg.cholesky(data_rhos))\n",
    "#Network outputs are the \"Tau\" Cholesky decomposition (in a NN-friendly version \"Flat\") of the coherence matrix Rho.\n",
    "\n",
    "#Example to explain the data form\n",
    "print(\"First coherency matrix:\")\n",
    "print(data_rhos[0].round(3))\n",
    "print(\"\")\n",
    "print(\"Its Cholesky decomposition in the form of a triangular matrix, Tau:\")\n",
    "print(np.linalg.cholesky(data_rhos[0]).round(3))\n",
    "print(\"\")\n",
    "print(\"The 4 real-valued elements of the Cholesky decomposition, Flat:\")\n",
    "print(data_out[0].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1f34cc-6bd0-4ebb-8902-6b88c041a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly shuffling the dataset. Keeping the position relevance. I.e., data_in[i] corresponds to data_out[i] after shuffle.\n",
    "seed(18)\n",
    "np.random.shuffle(data_in)\n",
    "seed(18)\n",
    "np.random.shuffle(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e5580a-e8bb-4bca-95e7-344212d10f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data:   (336000, 7)\n",
      "Number of validation data: (168000, 7)\n",
      "Number of test data:       (96000, 7)\n"
     ]
    }
   ],
   "source": [
    "#Separating the dataset into training, validation, and test set\n",
    "first_borderline = (number_of_repetitions+1) * 16000\n",
    "second_borderline = (number_of_repetitions+1) * 24000\n",
    "\n",
    "train_in = data_in[:first_borderline]\n",
    "train_out = data_out[:first_borderline]\n",
    "#Training set was used to train the network\n",
    "\n",
    "val_in = data_in[first_borderline:second_borderline]\n",
    "val_out = data_out[first_borderline:second_borderline]\n",
    "#Validation set was used to optimize the network\n",
    "\n",
    "test_in = data_in[second_borderline:]\n",
    "test_out = data_out[second_borderline:]\n",
    "#Test set is used now to evaluate the performance\n",
    "\n",
    "print(\"Number of training data:  \", train_in.shape)\n",
    "print(\"Number of validation data:\", val_in.shape)\n",
    "print(\"Number of test data:      \", test_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9cea0-bdd1-45b6-b5e7-2a932f258259",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "#### Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0922d8-4e74-4052-8659-401ba804d560",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 250)               2000      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 250)              1000      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 250)              1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 250)              1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 250)               62750     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 250)              1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 1004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 195,254\n",
      "Trainable params: 193,254\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"Trained_models/Main_results_model.h5\", \n",
    "                   custom_objects={\"fidelity_metric\": fidelity_metric})\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca792101-dc5c-4251-a1ef-0f143919430b",
   "metadata": {},
   "source": [
    "#### Evaluating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105e6111-bae5-4534-ae25-2bb5b945f336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test_rho = Flat_to_Density(model.predict(test_in))       #Predicted polarization states in the form of coherence matrices\n",
    "true_test_rho = Flat_to_Density(test_out)                     #The targeted coherence matrices\n",
    "\n",
    "test_fidelity_array = Fidelity(true_test_rho, pred_test_rho)\n",
    "test_infidelity_array = (1-test_fidelity_array)\n",
    "#The fidelities and infidelities between predicted and targeted coherence matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2807e0f-4d41-46ce-8b76-8cbfd4aad3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infidelity values used in the main text:\n",
      "The average value:   0.000838\n",
      "The 10th percentile: 5.3e-05\n",
      "The 95th percentile: 0.001829\n"
     ]
    }
   ],
   "source": [
    "print(\"Infidelity values used in the main text:\")\n",
    "print(\"The average value:  \", np.mean(test_infidelity_array).round(6))\n",
    "print(\"The 10th percentile:\", np.quantile(test_infidelity_array, 0.1).round(6))\n",
    "print(\"The 95th percentile:\", np.quantile(test_infidelity_array, 0.9).round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0025-49a0-49c9-b906-23b0c5b0b13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
